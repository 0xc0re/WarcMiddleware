{"note":"Don't delete this file! It's used internally to help with page regeneration.","google":"","tagline":"WarcMiddleware lets users seamlessly download a mirror copy of a website.","body":"WarcMiddleware\r\n==============\r\nWarcMiddleware is a library that lets users save mirror backups of websites to\r\ntheir computer. It is an addon for the Python web crawler Scrapy that saves\r\nweb server transactions (requests and responses) into a Web ARChive (WARC) file\r\n(ISO 28500). The transactions can then be played back or viewed, similar\r\nto using Archive.org's WayBackMachine. The WARC format is a standard method of\r\nsaving these transactions.\r\n\r\nThere are two ways to use WarcMiddleware: (1) as a replacement ClientFactory or\r\n(2) as a DownloaderMiddleware. The former is recommended. As a ClientFactory,\r\nWarcMiddleware hooks into Scrapy's HTTP class and saves the raw requests and\r\nresponses. The DownloaderMiddleware version configures itself to save the\r\nrequests and responses that Scrapy sends it. The problem with this method is\r\nthat Scrapy does not pass along the raw data and some of it is lost along the\r\nway.\r\n\r\nPrerequisites\r\n=============\r\nWarcMiddleware requires:\r\n\r\n* [Scrapy](http://scrapy.org/)\r\n * [Python Imaging Library](http://www.pythonware.com/products/pil/)\r\n * [lxml](http://pypi.python.org/pypi/lxml/)\r\n * [Twisted](http://twistedmatrix.com/trac/)\r\n     * [zope.interface](http://pypi.python.org/pypi/zope.interface)\r\n     * [OpenSSL](http://slproweb.com/products/Win32OpenSSL.html)\r\n     * [pyOpenSSL](https://launchpad.net/pyopenssl)\r\n\r\nFor Windows, many of these packages can be downloaded from\r\n<http://www.lfd.uci.edu/~gohlke/pythonlibs/>.\r\n\r\nSimple examples\r\n===============\r\nThe entire github repository serves as an example project which will download\r\na website and save it as a WARC file. To try it, download the repository as a\r\nzip file and extract it. After installing the prerequisites listed above, run:\r\n\r\n    $ crawler.py --url http://www.eurogamer.net/\r\n\r\nScrapy will save the website into a file named out.warc.gz\r\n\r\nSitemap.xml archiving\r\n---------------------\r\nThe crawler supports downloading urls from a sitemap.xml file.\r\nAs an example, this can be used to download all the posts from a Blogspot site.\r\nTo get the sitemap.xml from a Blogspot site, append \"sitemap.xml\" to the url and\r\nsave the file.\r\n\r\n    http://dogs.blogspot.com/sitemap.xml\r\n\r\nCrawl the sitemap using:\r\n\r\n    $ crawler.py --sitemap sitemap.xml\r\n\r\nScrapy will save the website contents into a file name out.warc.gz\r\n\r\nMirroring a domain\r\n------------------\r\nThe crawler supports crawling an entire domain or many domains.\r\n\r\nIf crawling a single domain, use:\r\n\r\n    $ crawler.py --mirror --url http://example.com\r\n\r\nUsing --mirror is the same as using `--domains example.com`.\r\n\r\nFor multiple domains, the following will crawl anchor links on example.com that\r\nlead to example.com or to othersite.com. And from othersite.com, it will crawl\r\nanchor links that lead to either:\r\n\r\n    $ crawler.py --domains example.com,othersite.com --url http://example.com\r\n\r\nRegular Expression crawling\r\n---------------------------\r\nThe --accept and --reject parameters affect whether or not each anchor link on a\r\nsite is crawled. Each accepts a comma-separated list of regular expressions that\r\nshould either be crawled or never crawled. This does not affect downloading\r\nexternal assets such as images or CSS files.\r\n\r\nThis example will not crawl anchor links that contains the string \"/search/?\":\r\n\r\n    $ crawler.py --mirror --reject /search/\\? --url http://example.com\r\n\r\nHow to view WARC files\r\n======================\r\nAfter creating a WARC file, the contents can be played back allowing the user\r\nto view the saved website. One way to view the saved site is to use [warc-proxy](https://github.com/alard/warc-proxy).\r\nWarc-proxy creates a proxy that channels traffic from a web browser and responds\r\nto requests to view websites. Rather than sending the live website, warc-proxy\r\nsends back the saved website contents from the WARC file.\r\n\r\nUsage in other Scrapy projects\r\n==============================\r\nA Scrapy project is needed to use WarcMiddleware. To create one, from a command\r\nprompt run:\r\n\r\n    $ scrapy startproject crawltest\r\n\r\nThis will create a crawltest directory (dir) with another crawltest dir inside.\r\n\r\nAfter this, choose one of the following methods to use WarcMiddleware.\r\n\r\nWarcClientFactory\r\n-----------------\r\nCopy warcclientfactory.py and warcrecords.py next to scrapy.cfg in the outer\r\ncrawltest dir. Also copy over the hanzo dir to the outer dir.\r\n\r\nIn the inner dir, open settings.py and add the following lines to the bottom:\r\n\r\n    DOWNLOADER_HTTPCLIENTFACTORY = 'warcclientfactory.WarcHTTPClientFactory'\r\n\r\nThis will enable the custom WarcMiddleware ClientFactory. Additionally, create\r\na simple spider by copying the \r\n[simplespider.py](https://github.com/iramari/WarcMiddleware/blob/master/crawltest/spiders/simplespider.py)\r\nfile into the spiders dir.\r\n\r\nFinally, to start the spider, from a command prompt in the outer dir run:\r\n\r\n    $ scrapy crawl simplespider -a url=http://www.eurogamer.net/\r\n\r\nThis should output a WARC file named out.warc.gz\r\n\r\nDownloaderMiddleware\r\n--------------------\r\nCopy warcmiddleware.py and warcrecords.py next to scrapy.cfg in the outer\r\ncrawltest dir. Also copy over the hanzo dir to the outer dir.\r\n\r\nIn the inner dir, open settings.py and add the following lines to the bottom:\r\n\r\n    DOWNLOADER_MIDDLEWARES = {'warcmiddleware.WarcMiddleware': 820}\r\n\r\nThis will enable the WarcMiddleware. Additionally, create a simple spider by\r\ncopying the\r\n[simplespider.py](https://github.com/iramari/WarcMiddleware/blob/master/crawltest/spiders/simplespider.py)\r\nfile into the spiders dir.\r\n\r\nFinally, to start the spider, from a command prompt in the outer dir run:\r\n\r\n    $ scrapy crawl simplespider -a url=http://www.eurogamer.net/\r\n\r\nThis should output a WARC file named out.warc.gz\r\n","name":"WarcMiddleware"}