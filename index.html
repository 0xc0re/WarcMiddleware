<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>WarcMiddleware by iramari</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>WarcMiddleware</h1>
        <p>WarcMiddleware lets users seamlessly download a mirror copy of a website.</p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/iramari/WarcMiddleware" class="button fork"><strong>View On GitHub</strong></a>
        <div class="downloads">
          <span>Downloads:</span>
          <ul>
            <li><a href="https://github.com/iramari/WarcMiddleware/zipball/master" class="button">ZIP</a></li>
            <li><a href="https://github.com/iramari/WarcMiddleware/tarball/master" class="button">TAR</a></li>
          </ul>
        </div>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>WarcMiddleware</h1>

<p>WarcMiddleware is a library that lets users save mirror backups of websites to
their computer. It is an addon for the Python web crawler Scrapy that saves
web server transactions (requests and responses) into a Web ARChive (WARC) file
(ISO 28500). The transactions can then be played back or viewed, similar
to using Archive.org's WayBackMachine. The WARC format is a standard method of
saving these transactions.</p>

<p>There are two ways to use WarcMiddleware: (1) as a replacement ClientFactory or
(2) as a DownloaderMiddleware. The former is recommended. As a ClientFactory,
WarcMiddleware hooks into Scrapy's HTTP class and saves the raw requests and
responses. The DownloaderMiddleware version configures itself to save the
requests and responses that Scrapy sends it. The problem with this method is
that Scrapy does not pass along the raw data and some of it is lost along the
way.</p>

<h1>Prerequisites</h1>

<p>WarcMiddleware requires:</p>

<ul>
<li>
<a href="http://scrapy.org/">Scrapy</a>

<ul>
<li><a href="http://www.pythonware.com/products/pil/">Python Imaging Library</a></li>
<li><a href="http://pypi.python.org/pypi/lxml/">lxml</a></li>
<li>
<a href="http://twistedmatrix.com/trac/">Twisted</a>

<ul>
<li><a href="http://pypi.python.org/pypi/zope.interface">zope.interface</a></li>
<li><a href="http://slproweb.com/products/Win32OpenSSL.html">OpenSSL</a></li>
<li><a href="https://launchpad.net/pyopenssl">pyOpenSSL</a></li>
</ul>
</li>
</ul>
</li>
</ul><p>For Windows, many of these packages can be downloaded from
<a href="http://www.lfd.uci.edu/%7Egohlke/pythonlibs/">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a>.</p>

<h1>Simple examples</h1>

<p>The entire github repository serves as an example project which will download
a website and save it as a WARC file. To try it, download the repository as a
zip file and extract it. After installing the prerequisites listed above, run:</p>

<pre><code>$ crawler.py --url http://www.eurogamer.net/
</code></pre>

<p>Scrapy will save the website into a file named out.warc.gz</p>

<h2>Sitemap.xml archiving</h2>

<p>The crawler supports downloading urls from a sitemap.xml file.
As an example, this can be used to download all the posts from a Blogspot site.
To get the sitemap.xml from a Blogspot site, append "sitemap.xml" to the url and
save the file.</p>

<pre><code>http://dogs.blogspot.com/sitemap.xml
</code></pre>

<p>Crawl the sitemap using:</p>

<pre><code>$ crawler.py --sitemap sitemap.xml
</code></pre>

<p>Scrapy will save the website contents into a file name out.warc.gz</p>

<h2>Mirroring a domain</h2>

<p>The crawler supports crawling an entire domain or many domains.</p>

<p>If crawling a single domain, use:</p>

<pre><code>$ crawler.py --mirror --url http://example.com
</code></pre>

<p>Using --mirror is the same as using <code>--domains example.com</code>.</p>

<p>For multiple domains, the following will crawl anchor links on example.com that
lead to example.com or to othersite.com. And from othersite.com, it will crawl
anchor links that lead to either:</p>

<pre><code>$ crawler.py --domains example.com,othersite.com --url http://example.com
</code></pre>

<h2>Regular Expression crawling</h2>

<p>The --accept and --reject parameters affect whether or not each anchor link on a
site is crawled. Each accepts a comma-separated list of regular expressions that
should either be crawled or never crawled. This does not affect downloading
external assets such as images or CSS files.</p>

<p>This example will not crawl anchor links that contains the string "/search/?":</p>

<pre><code>$ crawler.py --mirror --reject /search/\? --url http://example.com
</code></pre>

<h1>How to view WARC files</h1>

<p>After creating a WARC file, the contents can be played back allowing the user
to view the saved website. One way to view the saved site is to use <a href="https://github.com/alard/warc-proxy">warc-proxy</a>.
Warc-proxy creates a proxy that channels traffic from a web browser and responds
to requests to view websites. Rather than sending the live website, warc-proxy
sends back the saved website contents from the WARC file.</p>

<h1>Usage in other Scrapy projects</h1>

<p>A Scrapy project is needed to use WarcMiddleware. To create one, from a command
prompt run:</p>

<pre><code>$ scrapy startproject crawltest
</code></pre>

<p>This will create a crawltest directory (dir) with another crawltest dir inside.</p>

<p>After this, choose one of the following methods to use WarcMiddleware.</p>

<h2>WarcClientFactory</h2>

<p>Copy warcclientfactory.py and warcrecords.py next to scrapy.cfg in the outer
crawltest dir. Also copy over the hanzo dir to the outer dir.</p>

<p>In the inner dir, open settings.py and add the following lines to the bottom:</p>

<pre><code>DOWNLOADER_HTTPCLIENTFACTORY = 'warcclientfactory.WarcHTTPClientFactory'
</code></pre>

<p>This will enable the custom WarcMiddleware ClientFactory. Additionally, create
a simple spider by copying the 
<a href="https://github.com/iramari/WarcMiddleware/blob/master/crawltest/spiders/simplespider.py">simplespider.py</a>
file into the spiders dir.</p>

<p>Finally, to start the spider, from a command prompt in the outer dir run:</p>

<pre><code>$ scrapy crawl simplespider -a url=http://www.eurogamer.net/
</code></pre>

<p>This should output a WARC file named out.warc.gz</p>

<h2>DownloaderMiddleware</h2>

<p>Copy warcmiddleware.py and warcrecords.py next to scrapy.cfg in the outer
crawltest dir. Also copy over the hanzo dir to the outer dir.</p>

<p>In the inner dir, open settings.py and add the following lines to the bottom:</p>

<pre><code>DOWNLOADER_MIDDLEWARES = {'warcmiddleware.WarcMiddleware': 820}
</code></pre>

<p>This will enable the WarcMiddleware. Additionally, create a simple spider by
copying the
<a href="https://github.com/iramari/WarcMiddleware/blob/master/crawltest/spiders/simplespider.py">simplespider.py</a>
file into the spiders dir.</p>

<p>Finally, to start the spider, from a command prompt in the outer dir run:</p>

<pre><code>$ scrapy crawl simplespider -a url=http://www.eurogamer.net/
</code></pre>

<p>This should output a WARC file named out.warc.gz</p>
      </section>
      <footer>
        <p>Project maintained by <a href="https://github.com/iramari">iramari</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="http://twitter.com/#!/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>